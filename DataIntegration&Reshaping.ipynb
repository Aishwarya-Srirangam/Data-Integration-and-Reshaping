{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assessment 1\n",
    "#### Student Name: Aishwarya Srirangam Murali \n",
    "#### Student ID: 29999715\n",
    "\n",
    "Date: 18/11/2020\n",
    "\n",
    "\n",
    "Libraries used:\n",
    "* pandas 0.19.2 (for data frame, included in Anaconda Python 3.6) \n",
    "* re 2.2.1 (for regular expression, included in Anaconda Python 3.6) \n",
    "* tabula to read tables in PDF)\n",
    "* geopandas (Geometric operations are performed by shapely. Geopandas further depends on fiona for file access and descartes and matplotlib for plotting)\n",
    "* math (for mathematical functions)\n",
    "* numpy (adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays) \n",
    "* potly (to make line plots, scatter plots, area charts, bar charts, error bars, box plots, histograms, heatmaps, subplots, multiple-axes, polar charts, and bubble charts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tabula-py --user\n",
    "!pip install gtfs_kit --user\n",
    "!pip install geopandas --user\n",
    "!pip install plotly --user\n",
    "!pip install pandas --user\n",
    "!pip install numpy --user\n",
    "!pip install Shapely --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tabula\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "from shapely.geometry import Point, Polygon\n",
    "from math import radians, cos, sin, asin, sqrt \n",
    "from collections import Counter \n",
    "\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def ls(path): return list(path.iterdir())\n",
    "\n",
    "import zipfile\n",
    "\n",
    "def extract_file(path_to_zip, extract_to):  \n",
    "    with zipfile.ZipFile(path_to_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path('29999715.zip').exists():\n",
    "    extract_file('29999715.zip', '29999715')\n",
    "path = Path('./29999715/')\n",
    "ls(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true
   },
   "source": [
    "## Victoria Suburbs Boundary data\n",
    "* Reading the vic_suburb_boundary shape file \n",
    "* Checking for duplicates to check if the suburbs are repeated \n",
    "* Utilities to plot the points on victoria suburbs map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if Path('vic_suburb_boundary.zip').exists(): \n",
    "    extract_file('vic_suburb_boundary.zip', 'vic_suburb_boundary')\n",
    "ls(Path('vic_suburb_boundary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vic_sub = gpd.read_file('vic_suburb_boundary/VIC_LOCALITY_POLYGON_shp.shp')\n",
    "vic_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vic_sub.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#check for duplicates\n",
    "for c in vic_sub.columns:\n",
    "    print(c, (vic_sub.drop_duplicates([c]).shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "No duplicates in the geometry column, that means, the suburbs are not repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     7
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Utilities to plot on map\n",
    "\n",
    "def add_pos_col(df):\n",
    "    df.loc[:, \"geometry\"] = df[[\"lng\", \"lat\"]].apply(lambda p: Point(p[0], p[1]), axis=1)\n",
    "    \n",
    "def get_geodf(df, epsg1=4283, epsg2=None):\n",
    "    add_pos_col(df)\n",
    "    return gpd.GeoDataFrame(df).set_geometry('geometry')\n",
    "\n",
    "def plot_on_map(base_map, pts=None, ax=None, figsize=(10, 10), map_options={\"color\":\"gray\"}, \n",
    "                points_options = {\"column\":None, \"legend\":True, \"color\":\"blue\"}, filename=None):\n",
    "    \n",
    "    if ax==None: fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    base_map = base_map.copy()\n",
    "    base_map[\"index\"] = base_map.index\n",
    "    base_map.plot(ax=ax, **map_options)\n",
    "    if pts is not None: \n",
    "        pts = pts.copy()\n",
    "        get_geodf(pts).plot(ax=ax, **points_options)\n",
    "    if filename: \n",
    "        print(f\"Saving {filename}\")\n",
    "        plt.savefig(filename, figure=fig, bbox_inches='tight', pad_inches = 0.2)\n",
    "    return ax          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_on_map(vic_sub)\n",
    "plt.title('Victoria Suburbs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true
   },
   "source": [
    "## Hospital Data\n",
    "* Reading the hospitals.pdf file \n",
    "* Checking for duplicates \n",
    "* Checking if the ids are right "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hospitals = tabula.read_pdf(path/\"hospitals.pdf\", pages='all')\n",
    "hospitals = pd.concat([df[['id', 'lat', 'lng', 'name']] for df in hospitals[:5]]) #concat all data from the 5 pages of the pdf file\n",
    "hospitals = hospitals.dropna(how='all').reset_index(drop=True) #remove the empty rows\n",
    "hospitals.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#check for duplicates\n",
    "for c in hospitals.columns:\n",
    "    print(c, hospitals.shape, (hospitals.drop_duplicates([c]).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c = Counter(np.array(hospitals['name'].str.lower()))\n",
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Checked with the pdf file, indeed there are 4 records with the name 'Waverley Private Hospital' and 2 records with the name 'Epsworth Eastern Hospital'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hospitals[hospitals['name'] == 'Waverley Private Hospital']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hospitals[hospitals['name'] == 'Epsworth Eastern Hospital']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "On google search, I found that only one of the Epsworth Eastern hospital is real, same with Waverley private hospital; However, since we are not allowed to use any external data, its impossible to find out which ones we are supposed to keep, so I'll keep them all. Important thing to note here is that this might end up affecting the nearest hospital and distance to nearest hospital columns in the final schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Hospitals that do not exist\n",
    "problematic_hospitals = [\"hospital_140\", \"hospital_053\", \"hospital_057\", \"hospital_170\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "id_pattern = re.compile(\"^hospital_[0-9]{3}$\")\n",
    "hospitals[hospitals['id'].apply(lambda i: not bool(id_pattern.match(i)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "No problems with id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hospitals[['lat', 'lng']].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Seems within range, no other problems with hospital data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hospitals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_on_map(vic_sub, hospitals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true
   },
   "source": [
    "## Shopping centers\n",
    "* Reading shopingcenters.xlsx\n",
    "* Checking for duplicates \n",
    "* Checking for repititions or missing values \n",
    "* Plotting to see how the values are distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "shp_center = pd.read_excel(path/'shopingcenters.xlsx', index_col=0)\n",
    "shp_center.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "shp_center.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#check for duplicate, columnwise\n",
    "for c in shp_center.columns:\n",
    "    print(c, (shp_center.drop_duplicates([c]).shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "No duplicates!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "id_pattern = re.compile(\"^SC_[0-9]{3}$\")\n",
    "shp_center[shp_center['sc_id'].apply(lambda i: not bool(id_pattern.match(i)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "no problems with sc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "shp_center[['lat', 'lng']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "shp_center[['lat', 'lng']].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_on_map(vic_sub, shp_center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Looks like some of these shopping centers are at the other end of the world, we can't possibly fix these but we can remove them. However, there was no direction in the 'FIT5196-S2-2020 assessment 3.pdf' about such errors, so I am keeping them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true
   },
   "source": [
    "## Supermarkets\n",
    "* Reading supermarkets.html\n",
    "* Checking if there are any irregularities with the id \n",
    "* Checking for duplicates \n",
    "* Checking for repititions or missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(path/'supermarkets.html') as f:\n",
    "    supermarkets = pd.read_html(f, index_col=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "supermarkets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "supermarkets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "id_pattern = re.compile(\"^S_[0-9]{3}$\")\n",
    "supermarkets[supermarkets['id'].apply(lambda i: not bool(id_pattern.match(i)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "No problems with the id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for c in supermarkets.columns:\n",
    "    print(c, (supermarkets.drop_duplicates([c]).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c = Counter(np.array(supermarkets['lat']))\n",
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "supermarkets[supermarkets['lat']==-37.86685]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This one supermarket is repeated, let's remove the S_227"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "supermarkets = supermarkets.drop_duplicates(['lat', 'lng', 'type'], keep='first').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "supermarkets[supermarkets['lat']==-37.86685]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c = Counter(np.array(supermarkets['lng']))\n",
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "supermarkets[supermarkets['lng']==144.75218]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "These two records with identical lng (144.75218) are far apart, so no problems there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_on_map(vic_sub, supermarkets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "supermarkets[['lat', 'lng']].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Again, some supermarkets have problematic lat, long, we can't possibly fix them, so I am keeping them as it is in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "supermarkets['type'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "No problems here either"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true
   },
   "source": [
    "## Real Estate\n",
    "* Loading the xml and the json files \n",
    "* Removing all the duplicates \n",
    "* Concatenating them based on their rows to obtain complete real state dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Fix the real state.xml file\n",
    "\n",
    "with open(path/'real_state.xml', 'r+') as f:\n",
    "    data = f.read()\n",
    "    \n",
    "with open(path/'real_state-fixed.xml', 'w') as f:\n",
    "    f.write(data.strip('b')[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "file = path/'real_state-fixed.xml'\n",
    "with open(file) as fp:\n",
    "    soup = BeautifulSoup(fp, 'xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cols = ['property_id', 'lat', 'lng', 'addr_street', 'price', 'property_type', 'year', 'bedrooms', \n",
    "        'bathrooms', 'parking_space']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for c in cols:\n",
    "    print(c, len(soup.findAll(c)), len(soup.findAll(c)[0].findChildren()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_tags = [t.name for t in soup.findAll('property_id')[0].findChildren()]\n",
    "xml_data = {t: {} for t in all_tags}\n",
    "\n",
    "for c in cols:\n",
    "    rows = soup.findAll(c)[0].findChildren()\n",
    "    for r in rows:\n",
    "        assert r.name in all_tags\n",
    "        xml_data[r.name][c] = r.text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xml_df = pd.DataFrame(xml_data).transpose()\n",
    "xml_df.reset_index(drop=True, inplace=True)\n",
    "xml_df = xml_df.drop_duplicates()\n",
    "xml_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xml_df['lat'] = xml_df['lat'].astype(float)\n",
    "xml_df['lng'] = xml_df['lng'].astype(float) \n",
    "for c in ['property_id', 'price', 'year', 'bedrooms', 'bathrooms', 'parking_space']:\n",
    "    xml_df[c] = xml_df[c].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(path/'real_state.json') as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "json_df = pd.DataFrame(json_data)\n",
    "json_df = json_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "json_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "common_ids = xml_df[xml_df['property_id'].isin(json_df['property_id'])]['property_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(common_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in common_ids:\n",
    "    a = list(xml_df[xml_df['property_id'] == i].transpose().to_dict().items())[0][1]\n",
    "    b = list(json_df[json_df['property_id'] == i].transpose().to_dict().items())[0][1]\n",
    "    if a != b:\n",
    "        print('-'*50)\n",
    "        for k, v in a.items():\n",
    "            if b[k] != v:\n",
    "                print(k, round(v, 6), round(b[k], 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The records with common property id are exactly the same, so we just have to combine the two data frames, while only keeping the common ids from one of them, so in total there will be 1002 (json) + 1005 (xml) - 25 (common) = 1982 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "real_state = pd.concat([xml_df, json_df[~ json_df['property_id'].isin(common_ids)]]).reset_index(drop=True)\n",
    "real_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "real_state.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for c in real_state.columns:\n",
    "    print(c, real_state.shape, (real_state.drop_duplicates([c]).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c = Counter(np.array(real_state['lat']))\n",
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "real_state[real_state['lat'].isin([o[0] for o in c.most_common(5)])].sort_values(by='lat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c = Counter(np.array(real_state['lng']))\n",
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "real_state[real_state['lng'].isin([o[0] for o in c.most_common(5)])].sort_values(by='lng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c = Counter(np.array(real_state['addr_street']))\n",
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "real_state[real_state['addr_street'].isin([o[0] for o in c.most_common(5)])].sort_values(by='addr_street')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We have already removed all the duplciates, and looking at the repititions in the lat/lng/addr_street column tells us that there aren't any rows with different property id but exact same everything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(real_state['addr_street'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(real_state['addr_street'].str.lower().unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So no same add_street rows with different Capitalizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "real_state[['lat', 'lng']].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_on_map(vic_sub, real_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Looks like the dataset is just for Melbourne, and suburbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "id_pattern = re.compile(\"^[0-9]+$\")\n",
    "real_state[real_state['property_id'].astype(str).apply(lambda i: not bool(id_pattern.match(i)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "property_id okay too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true
   },
   "source": [
    "## GTFS data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Lets look at the structure of the zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "extract_file('GTFS_Melbourne_Train_Information.zip', 'GTFS_Melbourne_Train_Information')\n",
    "ls(ls(ls(Path('GTFS_Melbourne_Train_Information'))[0])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "so the files are in `GTFS_Melbourne_Train_Information/1. GTFS - Melbourne Train Information - From PTV (9 Oct 2015)/GTFS - Melbourne Train Information/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Feed(object):\n",
    "    def __init__(self, zip_file):\n",
    "        extract_file(zip_file, str(zip_file)[:-4])\n",
    "        self.files_path = ls(ls(Path('GTFS_Melbourne_Train_Information'))[0])[0]\n",
    "        print('All files: ', [f.name for f in ls(self.files_path)])\n",
    "        \n",
    "        self.agency = self.get_file('agency.txt')\n",
    "        self.calendar = self.get_file('calendar.txt')\n",
    "        self.calendar_dates = self.get_file('calendar_dates.txt')\n",
    "        self.routes = self.get_file('routes.txt')\n",
    "        self.shapes = self.get_file('shapes.txt')\n",
    "        self.stops = self.get_file('stops.txt')\n",
    "        self.stop_times = self.get_file('stop_times.txt')\n",
    "        self.trips = self.get_file('trips.txt')\n",
    "        \n",
    "    def get_file(self, filename):\n",
    "        return pd.read_csv(self.files_path/filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "feed = Feed('GTFS_Melbourne_Train_Information.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "feed.stops[['stop_lat', 'stop_lon']].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for c in feed.stops.columns:\n",
    "    print(c, (feed.stops.drop_duplicates([c]).shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## combine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = real_state.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "hidden": true
   },
   "source": [
    "### Shortest ids and distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Haversine\n",
    "def distance_in_kms(lat1, long1, lat2, long2): \n",
    "    lat1, long1, lat2, long2 = radians(lat1), radians(long1), radians(lat2), radians(long2)    \n",
    "    d = sin((lat2 - lat1) / 2)**2 + cos(lat1) * cos(lat2) * sin((long2-long1) / 2)**2\n",
    "    d = 2 * asin(sqrt(d)) * 6378\n",
    "    return d\n",
    "\n",
    "\n",
    "def min_distance(r, dist_from):\n",
    "    plat, plng = r['lat'], r['lng']\n",
    "    dist_from['dist'] = dist_from.apply(lambda x: distance_in_kms(float(x['lat']), float(x['lng']), plat, plng), axis=1)\n",
    "    sorted_dist_from = dist_from.sort_values(by='dist').reset_index(drop=True)\n",
    "    if sorted_dist_from.loc[0]['dist'] == sorted_dist_from.loc[1]['dist']:\n",
    "        print('-'*100)\n",
    "        print(r)\n",
    "        print('-'*50)\n",
    "        print(sorted_dist_from.loc[0])\n",
    "        print('-'*50)\n",
    "        print(sorted_dist_from.loc[1])\n",
    "    assert (sorted_dist_from.loc[0]['lat'] < sorted_dist_from.loc[0]['lng'])\n",
    "    assert (sorted_dist_from.loc[0]['lat'] < 30)\n",
    "    assert (sorted_dist_from.loc[0]['lng'] > 140)\n",
    "    return sorted_dist_from.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Add the Shopping_center_id for the nearest shopping center with Distance_to_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['Shopping_center_id'] = df.apply(lambda r: min_distance(r, shp_center.copy())['sc_id'], axis=1)\n",
    "df['Distance_to_sc'] = df.apply(lambda r: round(min_distance(r, shp_center.copy())['dist'], 3), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Add the Hospital_id for the nearest hospital with Distance_to_hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['Hospital_id'] = df.apply(lambda r: min_distance(r, hospitals.copy())['id'], axis=1)\n",
    "df['Distance_to_hospital'] = df.apply(lambda r: round(min_distance(r, hospitals.copy())['dist'], 3), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df[df['Hospital_id'].isin(problematic_hospitals)].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is problematic, but since we can't change anything in the hospitals file, we will just let it be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now, Add the Supermarket_id for the nearest supermarket with Distance_to_supermaket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df['Supermarket_id'] = df.apply(lambda r: min_distance(r, supermarkets.copy())['id'], axis=1)\n",
    "df['Distance_to_supermaket'] = df.apply(lambda r: round(min_distance(r, supermarkets.copy())['dist'], 3), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Add the Train_station_id for the nearest train station with Distance_to_train_station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_stops = feed.stops.copy()\n",
    "train_stops['lat'] = train_stops['stop_lat']\n",
    "train_stops['lng'] = train_stops['stop_lon']\n",
    "train_stops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['Train_station_id'] = df.apply(lambda r: min_distance(r, train_stops.copy())['stop_id'], axis=1)\n",
    "df['Distance_to_train_station'] = df.apply(lambda r: round(min_distance(r, train_stops.copy())['dist'], 3), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "hidden": true
   },
   "source": [
    "### Suburb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's find and all suburbs now, the `VIC_LOCA_2` column represents the suburb name in the vic_suburb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def find_suburb(r, suburbs):\n",
    "    plat, plng = r['lat'], r['lng']\n",
    "    p = Point(float(plng), float(plat))\n",
    "    suburb = []\n",
    "    suburbs['belong_here'] = suburbs.apply(lambda g: (g.geometry.contains(p) or g.geometry.touches(p)), axis=1)\n",
    "    suburb = suburbs[suburbs['belong_here'] == True]\n",
    "    assert len(suburb) <= 1\n",
    "    if len(suburb) == 1:\n",
    "        return suburb.iloc[0]\n",
    "    return \"not available\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# A few tests, compared with google maps\n",
    "%time assert find_suburb(df.iloc[0], vic_sub.copy())['VIC_LOCA_2'] == \"MOOROOLBARK\"\n",
    "%time assert find_suburb(df.iloc[5], vic_sub.copy())['VIC_LOCA_2'] == \"MENTONE\"\n",
    "%time assert find_suburb(df.iloc[199], vic_sub.copy())['VIC_LOCA_2'] == \"LALOR\"\n",
    "%time assert find_suburb(df.iloc[1599], vic_sub.copy())['VIC_LOCA_2'] == \"SEDDON\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['suburb'] = df.apply(lambda r: find_suburb(r, vic_sub.copy())[\"VIC_LOCA_2\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df[df['suburb'] == 'not available'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "All suburbs are available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### travel_min_to_CBD and Transfer_flag columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "feed.stops[feed.stops['stop_name'].str.contains('Flinders Street')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "possible_trips = list(feed.trips[feed.trips['service_id'] == 'T0']['trip_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We only want trips with service id T0 because it runs on all days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_in_min(t):\n",
    "    h, m, s = list(map(int, t.split(':')))\n",
    "    return (h*60 + m + s/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stop_times = feed.stop_times\n",
    "stop_times['arrival_min'] = stop_times['arrival_time'].apply(get_in_min)\n",
    "stop_times['departure_min'] = stop_times['departure_time'].apply(get_in_min)\n",
    "stop_times['stop_id'] = stop_times['stop_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the below code, \n",
    "\n",
    "* Stop times to filter the trips with stops which is nearest to the property and the stop id of flinder's street \n",
    "* Filtering out the trips that depart between 7am to 9pm from the stop_id and whose arrival times at the city centre are after the departure times from the stop nearest to the property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def trips_stop2city(stop_id):\n",
    "    trips_from_stop = stop_times[(stop_times['stop_id'] == str(stop_id)) & (stop_times['departure_min'] >=420) & (stop_times['departure_min'] <= 540) & (stop_times['trip_id'].isin(possible_trips))].reset_index(drop=True)    \n",
    "    trips_from_city = stop_times[(stop_times['stop_id'] == \"19854\")]\n",
    "    common_trips = list(trips_from_city[trips_from_city['trip_id'].isin(list(trips_from_stop['trip_id']))]['trip_id'])\n",
    "    \n",
    "    trips_from_city.index = trips_from_city['trip_id']\n",
    "    trips_from_stop.index = trips_from_stop['trip_id']\n",
    "    \n",
    "    \n",
    "    #Filter common trips in same sequence\n",
    "    trips_from_city = trips_from_city.loc[common_trips].reset_index(drop=True)\n",
    "    trips_from_stop = trips_from_stop.loc[common_trips].reset_index(drop=True)\n",
    "    \n",
    "    assert list(trips_from_stop['trip_id']) == list(trips_from_city['trip_id'])\n",
    "    \n",
    "    fil = (trips_from_stop['departure_min'] <= trips_from_city['departure_min']) & (trips_from_stop['stop_sequence'].astype(int) < trips_from_city['stop_sequence'].astype(int))\n",
    "    trips_from_stop_to_city = trips_from_stop[fil].reset_index(drop=True)\n",
    "    trips_to_city_from_stop = trips_from_city[fil].reset_index(drop=True)\n",
    "    return trips_from_stop_to_city, trips_to_city_from_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for stop_id in tqdm_notebook(df['Train_station_id']):\n",
    "    trips_from_stop_to_city, _ = trips_stop2city(stop_id)\n",
    "    if len(trips_from_stop_to_city) == 0:\n",
    "        print(stop_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This means that there is at least one direct trip (departing between 7am to 9am)that with service id T0 between all stations and the Flinder's Street except for the stops 20027 and 19854 (this one is flinder's street itself), we can ignore the station 20027 as it appears only 4 times in the total dataset, so we have only considered the direct trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stop_to_city, city_from_stop  = trips_stop2city('19948')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_travel_time(r, stop_times):\n",
    "    stop_id = r['Train_station_id']\n",
    "    if str(stop_id) == '19854':\n",
    "        return 0, 0 #Nearest Stop is city, so no tran\n",
    "    else:\n",
    "        stop_to_city, city_from_stop  = trips_stop2city(stop_id)\n",
    "        if len(stop_to_city) == 0:\n",
    "            return 0, -1 #Default values \n",
    "        else:\n",
    "            travel_times = city_from_stop['arrival_min'] - stop_to_city['departure_min']\n",
    "            assert (city_from_stop['departure_min'] < 24*60).sum() == city_from_stop.shape[0]\n",
    "            return np.floor(travel_times.mean()), 0 #all direct trips only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['travel_min_to_CBD'] = df.apply(lambda r: get_travel_time(r, stop_times)[0], axis=1)\n",
    "df['Transfer_flag'] = df.apply(lambda r: get_travel_time(r, stop_times)[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df[df['Transfer_flag'] == -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Only four rows were left with default values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Save the combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "req_columns = ['property_id', 'lat', 'lng', 'addr_street',\n",
    "                'suburb', 'property_type', 'year', 'bedrooms',\n",
    "                'bathrooms', 'parking_space', 'Shopping_center_id', \n",
    "                'Distance_to_sc', 'Train_station_id', 'Distance_to_train_station',\n",
    "                'travel_min_to_CBD', 'Transfer_flag', 'Hospital_id',\n",
    "                'Distance_to_hospital', 'Supermarket_id', 'Distance_to_supermaket']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df[req_columns].to_csv('29999715_A3_solution.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('29999715_A3_solution.csv')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for c in submission.columns:\n",
    "    assert c in req_columns\n",
    "    \n",
    "for r in req_columns:\n",
    "    assert r in submission.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import plotly.express as px\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_variables = ['Distance_to_sc', 'travel_min_to_CBD', 'Distance_to_hospital']\n",
    "target_variable = 'price'\n",
    "variables_of_interes = predictor_variables + [target_variable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[predictor_variables].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These predictor variables have been measured at different scales. i.e., `Distance_to_sc` in km, `travel_min_to_CBD` in minutes, and `Distance to hsopital` in km, therefore, we need to scale them first to make comparisons, it is generally not required to transform the output variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Std and MinMax scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = preprocessing.StandardScaler().fit(df[predictor_variables])\n",
    "std_values = std_scaler.transform(df[predictor_variables])\n",
    "std_values, std_values.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_predictor_variables = [f'std_{v}' for v in predictor_variables]\n",
    "for i, v in enumerate(std_predictor_variables):\n",
    "    df[v] = std_values[:, i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plan to check the normality of the distributions of the variable and hence we many need to transform the variables. It will help in transformation if our variables have values that are strictly positive. Therefore, let's use the `feature_range` option in the Min Max scaler to ensure that we have strictly positive values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_scaler = preprocessing.MinMaxScaler(feature_range=(1, 2)).fit(df[predictor_variables])\n",
    "minmax_values = minmax_scaler.transform(df[predictor_variables])\n",
    "\n",
    "minmax_predictor_variables = [f'minmax_{v}' for v in predictor_variables]\n",
    "for i, v in enumerate(minmax_predictor_variables):\n",
    "    df[v] = minmax_values[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[predictor_variables + std_predictor_variables + minmax_predictor_variables].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to apply boxcox transformation, we need to take the minmax normalized data as its values start from zero and boxcox transformation requires values>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref = 'minmax'\n",
    "independant_vars_dict = {'': predictor_variables, 'std': std_predictor_variables,\n",
    "                        'minmax': minmax_predictor_variables}\n",
    "\n",
    "independent_vars = independant_vars_dict[pref]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Normality of the continous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Algorithms like linear regression and logistic regression explicitly assume the real-valued variables have a Gaussian distribution and therefore, its important that we check if our input and output variables have gaussian distribution. Let's also compare with the distributions of the box-cox and square root (sqrt) transformed values of the respective variables. We have also looked at skew values and their changes as we use box-cox and sqrt transformation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     21,
     40
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def transform_boxcox(og_data, name):\n",
    "    tfmd_data, tfm_lambda = stats.boxcox(og_data)\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    kwargs_for_plotting = dict(hist=True, kde=True, \n",
    "                           kde_kws = {'shade': True, 'linewidth': 2},\n",
    "                           color='blue')\n",
    "    \n",
    "    sns.distplot(og_data, label='Original data', ax=ax[0], **kwargs_for_plotting)\n",
    "    sns.distplot(tfmd_data, label='BoxCox Transformed data', ax=ax[1], **kwargs_for_plotting)\n",
    "    \n",
    "    ax[0].set_xlabel(name); ax[1].set_xlabel(name);\n",
    "    plt.legend(loc = \"upper right\") \n",
    "    fig.set_figheight(5) \n",
    "    fig.set_figwidth(10) \n",
    "    \n",
    "    print('Lambda value: ', tfm_lambda)\n",
    "    print('Change in skew', stats.skew(og_data), \"-->\", stats.skew(tfmd_data))\n",
    "    return tfmd_data, tfm_lambda\n",
    "\n",
    "\n",
    "def transform_sqrt(og_data, name):\n",
    "    tfmd_data = np.sqrt(og_data)\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    kwargs_for_plotting = dict(hist=True, kde=True, \n",
    "                           kde_kws = {'shade': True, 'linewidth': 2},\n",
    "                           color='blue')\n",
    "    \n",
    "    sns.distplot(og_data, label='Original data', ax=ax[0], **kwargs_for_plotting)\n",
    "    sns.distplot(tfmd_data, label='Sqrt Transformed data', ax=ax[1], **kwargs_for_plotting)\n",
    "    \n",
    "    ax[0].set_xlabel(name); ax[1].set_xlabel(name);\n",
    "    plt.legend(loc = \"upper right\") \n",
    "    fig.set_figheight(5) \n",
    "    fig.set_figwidth(10) \n",
    "    \n",
    "    print('Change in skew', stats.skew(og_data), \"-->\", stats.skew(tfmd_data))\n",
    "    return tfmd_data, None\n",
    "\n",
    "\n",
    "def transform_log(og_data, name):\n",
    "    tfmd_data = np.log(og_data)\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    kwargs_for_plotting = dict(hist=True, kde=True, \n",
    "                           kde_kws = {'shade': True, 'linewidth': 2},\n",
    "                           color='blue')\n",
    "    \n",
    "    sns.distplot(og_data, label='Original data', ax=ax[0], **kwargs_for_plotting)\n",
    "    sns.distplot(tfmd_data, label='Log Transformed data', ax=ax[1], **kwargs_for_plotting)\n",
    "    \n",
    "    ax[0].set_xlabel(name); ax[1].set_xlabel(name);\n",
    "    plt.legend(loc = \"upper right\") \n",
    "    fig.set_figheight(5) \n",
    "    fig.set_figwidth(10) \n",
    "    \n",
    "    print('Change in skew', stats.skew(og_data), \"-->\", stats.skew(tfmd_data))\n",
    "    return tfmd_data, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df[\"bc_\" + independent_vars[0]] = transform_boxcox((df[independent_vars[0]]), independent_vars[0])[0]\n",
    "df[\"bc_\" + independent_vars[1]] = transform_boxcox((df[independent_vars[1]]), independent_vars[1])[0]\n",
    "df[\"bc_\" + independent_vars[2]] = transform_boxcox((df[independent_vars[2]]), independent_vars[2])[0]\n",
    "df['bc_price'] = transform_boxcox(df['price'], 'price')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So the boxcox transformation helps a lot in transforming the distributions to normal for the variables of with Distance_to_sc, Distance_to_hospital, travel_min_to_CBD and price. There are significant changes in the values of skew and the distribution of the transformed values look much more normal compared to that of the non-transfomred values. Let's also try the sqrt transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df[\"sqrt_\" + independent_vars[0]] = transform_sqrt(df[independent_vars[0]], independent_vars[0])[0]\n",
    "df[\"sqrt_\" + independent_vars[1]] = transform_sqrt(df[independent_vars[1]], independent_vars[1])[0]\n",
    "df[\"sqrt_\" + independent_vars[2]] = transform_sqrt(df[independent_vars[2]], independent_vars[2])[0]\n",
    "df['sqrt_price'] = transform_sqrt(df['price'], 'price')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So the square root transformation helps in transforming the distributions of with Distance_to_sc, Distance_to_hospital. But it is not as good while transforming the distribution of the output variable Price and the input variable tavel_min_to_CBD. Let us also consider the log transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df[\"log_\" + independent_vars[0]] = transform_log(df[independent_vars[0]], independent_vars[0])[0]\n",
    "df[\"log_\" + independent_vars[1]] = transform_log(df[independent_vars[1]], independent_vars[1])[0]\n",
    "df[\"log_\" + independent_vars[2]] = transform_log(df[independent_vars[2]], independent_vars[2])[0]\n",
    "df['log_price'] = transform_log(df['price'], 'price')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Neither log nor sqrt transformations are as good as the box-cox transformation. This is expected as the box-cox transformation is more adaptive. So let us use the box-cox transformation only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A linear model will try to fit a straight line through the data points given. Therefore, it is important to ensure that there is a linear relationship between the independent variable and the dependent variables. We plot each of the independ variable against he dependent variable to see if there is a linear relationship. We also plot a trendline using ols regression and look the r-squared value of that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "r2_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(df, x=\"bc_\"+independent_vars[0], y=\"bc_price\", trendline=\"ols\")\n",
    "fig.show()\n",
    "\n",
    "results = px.get_trendline_results(fig)\n",
    "results = results.iloc[0][\"px_fit_results\"].summary()\n",
    "r2_values.append(float(pd.DataFrame(results.tables[0]).iloc[0, 3].data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(df, x=\"bc_\"+independent_vars[1], y=\"bc_price\", trendline=\"ols\")\n",
    "fig.show()\n",
    "\n",
    "\n",
    "results = px.get_trendline_results(fig)\n",
    "results = results.iloc[0][\"px_fit_results\"].summary()\n",
    "r2_values.append(float(pd.DataFrame(results.tables[0]).iloc[0, 3].data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(df, x=\"bc_\"+independent_vars[2], y=\"bc_price\", trendline=\"ols\")\n",
    "fig.show()\n",
    "\n",
    "\n",
    "results = px.get_trendline_results(fig)\n",
    "results = results.iloc[0][\"px_fit_results\"].summary()\n",
    "r2_values.append(float(pd.DataFrame(results.tables[0]).iloc[0, 3].data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "r2_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There is a linear relationship between the predictor variables and the target variable. This linear relationship is more pronounced in case of the input variables Distance_to_hospital and travel_min_to_CBD (Both min max normalized and box cox transformed). Therfore the condition of Linearity is satisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Normality of the Residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Build a linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now, lets build a model and look at the residuals to ensure the condition of normality is maintained, we will use the box cox transformed values of min max normalized input variables with an offset value of 1, and the box cox transformed output variable price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_results(model, X, y):\n",
    "    preds = model.predict(X)\n",
    "    results = pd.DataFrame({'Actual': y, 'Predicted': preds})\n",
    "    results['Residuals'] = np.abs(results['Actual']) - np.abs(results['Predicted'])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's try one model with transfomred output variable and one model with non-transformed output variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Model with transformed output variable\n",
    "\n",
    "X, y = df[[\"bc_\"+iv for iv in independent_vars]], df['bc_price']\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "r2 = model.score(X, y)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Model with non transformed output variable\n",
    "\n",
    "X, y_non_tfmd = df[[\"bc_\"+iv for iv in independent_vars]], df['price']\n",
    "\n",
    "model_non_tfmd = LinearRegression()\n",
    "model_non_tfmd.fit(X, y_non_tfmd)\n",
    "\n",
    "r2 = model_non_tfmd.score(X, y_non_tfmd)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Check normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_hist(data):\n",
    "    kwargs_for_plotting = dict(hist=True, kde=True, \n",
    "                           kde_kws = {'shade': True, 'linewidth': 2},\n",
    "                           color='blue')\n",
    "    \n",
    "    sns.distplot(data, **kwargs_for_plotting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results = get_results(model, X, y)\n",
    "# results['Residuals'].plot.hist()\n",
    "plot_hist(results['Residuals'])\n",
    "plt.xlabel('Residuals')\n",
    "plt.title('Model with both input and output variables transformed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results = get_results(model_non_tfmd, X, y_non_tfmd)\n",
    "plot_hist(results['Residuals'])\n",
    "plt.xlabel('Residuals')\n",
    "plt.title('Model with only input variables transformed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "From the above two graphs it is clear that the model with both input and outputs transformed satisfies the normality assumption better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### References \n",
    "* Tutorials — pandas 0.15.2 documentation. (2020). Retrieved 18 November 2020, from https://pandas.pydata.org/pandas-docs/version/0.15/tutorials.html\n",
    "* tabula-py. (2020). Retrieved 18 November 2020, from https://pypi.org/project/tabula-py/\n",
    "* GeoPandas 0.8.0 — GeoPandas 0.8.0 documentation. (2020). Retrieved 18 November 2020, from https://geopandas.org/\n",
    "* math — Mathematical functions — Python 3.9.0 documentation. (2020). Retrieved 18 November 2020, from https://docs.python.org/3/library/math.html\n",
    "* Overview — NumPy v1.19 Manual. (2020). Retrieved 18 November 2020, from https://numpy.org/doc/stable/ \n",
    "* Plotly Python Graphing Library. (2020). Retrieved 18 November 2020, from https://plotly.com/python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
